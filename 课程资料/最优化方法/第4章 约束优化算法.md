# 第4章 约束优化算法

## 约束优化问题简介

### 一般约束优化问题
在本章中，我们将学习求解以下形式的约束优化问题：
$$
\min_{x} f_0(x) \quad \text{s.t.} \quad x \in \mathcal{X}
$$
其中 $\mathcal{X} \subseteq \mathbb{R}^n$ 是问题的可行域。

### 核心难点
由于约束的存在，无约束优化算法（如梯度下降法）无法直接应用。
*   **可行性问题**：沿着负梯度方向更新所得的点不一定在可行域 $\mathcal{X}$ 内。
    $$
    x^{k+1} = x^k - \alpha_k \nabla f_0(x^k), \quad \text{但是 } x^{k+1} \notin \mathcal{X} \text{ 可能发生}
    $$
*   **最优性条件改变**：目标函数的梯度在最优解处不一定是零向量。
    $$
    x^* \in \arg\min_{x \in \mathcal{X}} f_0(x), \quad \text{但是 } \nabla f_0(x^*) \neq 0 \text{ 依然可能}
    $$

**核心问题**：如何处理约束？

### 处理约束的思路
1.  **变量消去法**：对于简单的约束（如线性等式约束），可以尝试解出部分变量，代入目标函数，将问题转化为无约束优化问题。但这通常只适用于非常特殊的情况，求解非线性方程组极为困难。
2.  **转化方法**：将约束问题转化为一系列的无约束优化问题来求解。这是本章介绍的主要方法，例如罚函数法。

## 罚函数法
罚函数法的核心思想是将约束“融入”到目标函数中。通过对违反约束的点施加“惩罚”，将一个约束优化问题近似地转化为一个无约束优化问题。

### 二次罚函数法 (针对等式约束)

考虑等式约束优化问题 (P)：
$$
\min_x f_0(x) \quad \text{s.t.} \quad h_i(x) = 0, \quad i=1, \dots, p
$$

#### 定义1 (二次罚函数)
问题(P)的**二次罚函数**定义为：
$$
P_E(x, \sigma) = f_0(x) + \frac{\sigma}{2} \sum_{i=1}^p |h_i(x)|^2
$$
其中，常数 $\sigma > 0$ 称为**罚因子**或**罚参数**。

> **直观理解**：
> *   我们构造了一个新的无约束目标函数 $P_E(x, \sigma)$。
> *   当一个点 $x$ 满足约束时 ($h_i(x)=0$)，惩罚项 $\frac{\sigma}{2} \sum |h_i(x)|^2$ 为零，此时 $P_E(x, \sigma) = f_0(x)$。
> *   当一个点 $x$ 违反约束时 ($h_i(x) \neq 0$)，惩罚项为一个正数，这个点在新的目标函数中的值就被“惩罚”性地增大了。
> *   罚因子 $\sigma$ 控制着惩罚的“力度”。$\sigma$ 越大，对违反约束的惩罚就越重，无约束问题的最优解就会被“逼”得越靠近原问题的可行域。

#### Algorithm1 (二次罚函数法)
1.  **初始化**：给定初始点 $x_0$，初始罚参数 $\sigma_0 > 0$，罚因子增长系数 $\rho > 1$。令 $k=0$。
2.  **迭代**：
    **while** 未达到收敛准则 **do**
    *   以 $x^k$ 为初始点，求解无约束问题：
        $$
        x^{k+1} \in \arg\min_x P_E(x, \sigma_k)
        $$
    *   增大罚参数：$\sigma_{k+1} = \rho \sigma_k$
    *   $k \leftarrow k+1$
    **end**

#### 定理1 (二次罚函数法的收敛性)
假设对任意 $\sigma > 0$，罚函数 $P_E(x, \sigma)$ 的全局最优解都存在。设罚因子序列 $\{\sigma_k\}$ 单调递增趋于无穷。若 $x^{k+1}$ 是 $P_E(x, \sigma_k)$ 的一个全局最优解，则序列 $\{x^k\}$ 的任何一个聚点（极限点）$x^*$ 都是原问题(P)的全局最优解。

### 二次罚函数法的局限性
罚函数法虽然思想简单，但存在一个致命的缺陷：**数值不稳定性**。

考虑原问题(P)的KKT条件与罚函数子问题的KKT（梯度为0）条件：
*   **KKT-P** (原问题在最优解 $x^*$):
    $$
    \nabla f_0(x^*) + \sum_{i=1}^p \lambda_i^* \nabla h_i(x^*) = 0
    $$
*   **KKT-E** (罚函数子问题在最优解 $x^{k+1}$):
    $$
    \nabla f_0(x^{k+1}) + \sum_{i=1}^p \sigma_k h_i(x^{k+1}) \nabla h_i(x^{k+1}) = 0
    $$

为了让 $x^{k+1} \to x^*$，通过对比上述两个系统，我们期望：
$$
\sigma_k h_i(x^{k+1}) \to \lambda_i^*
$$
由于我们希望 $x^{k+1}$ 满足约束，即 $h_i(x^{k+1}) \to 0$，那么为了让乘积趋向于一个非零的 $\lambda_i^*$，就必须有：
$$
\sigma_k \to +\infty
$$

> **直观理解**：
> 理论上，我们需要将罚因子 $\sigma_k$ 增大到无穷大才能保证收敛到真正的最优解。但在数值计算中，一个非常大的 $\sigma_k$ 会使得罚函数 $P_E(x, \sigma_k)$ 的Hessian矩阵变得**病态 (ill-conditioned)**。这就像在优化一个极其狭窄和陡峭的山谷，对无约束优化算法来说，求解这样的问题非常困难，容易产生数值错误和溢出。

## 增广拉格朗日乘子法 (ALM)
为了克服罚函数法中罚因子需要趋于无穷的缺陷，增广拉格朗日乘子法 (Augmented Lagrangian Method, ALM)，又称乘子法，被提了出来。

**核心思想**：在罚函数的基础上，引入拉格朗日乘子，从而避免了将罚因子推向无穷。

### 针对等式约束问题的ALM

#### 增广拉格朗日函数
对于等式约束问题 (P)，其**增广拉格朗日函数**定义为：
$$
L_\sigma(x, \lambda) = f_0(x) + \sum_{i=1}^p \lambda_i h_i(x) + \frac{\sigma}{2} \sum_{i=1}^p |h_i(x)|^2
$$
> **直观理解**：这个函数是标准拉格朗日函数和二次罚函数的结合体。它既包含了乘子项 $\sum \lambda_i h_i(x)$（对偶信息），也包含了二次惩罚项。

#### ALM 算法框架
1.  **初始化**：给定 $x^0, \lambda^0, \sigma_0 > 0, \rho > 1$。
2.  **迭代**：
    **while** 未收敛 **do**
    *   **更新 x**：求解无约束子问题
        $$
        x^{k+1} \in \arg\min_x L_{\sigma_k}(x, \lambda^k)
        $$
    *   **更新 $\lambda$**：更新拉格朗日乘子
        $$
        \lambda_i^{k+1} = \lambda_i^k + \sigma_k h_i(x^{k+1}), \quad i=1, \dots, p
        $$
    *   **更新 $\sigma$**：更新罚因子（可选，也可以固定）
        $$
        \sigma_{k+1} = \rho \sigma_k
        $$
    **end**

#### ALM的构造思想与优势
再次对比KKT条件，ALM子问题的最优性条件是 $\nabla_x L_{\sigma_k}(x^{k+1}, \lambda^k) = 0$，即：
$$
\nabla f_0(x^{k+1}) + \sum_{i=1}^p (\lambda_i^k + \sigma_k h_i(x^{k+1})) \nabla h_i(x^{k+1}) = 0
$$
观察乘子更新公式 $\lambda_i^{k+1} = \lambda_i^k + \sigma_k h_i(x^{k+1})$，上式恰好是：
$$
\nabla f_0(x^{k+1}) + \sum_{i=1}^p \lambda_i^{k+1} \nabla h_i(x^{k+1}) = 0
$$
这与原问题的KKT条件形式上非常接近。这个巧妙的乘子更新规则使得 $\lambda^k$ 序列能够收敛到最优乘子 $\lambda^*$，即使 $\sigma_k$ 保持为一个有限的、足够大的常数。

**ALM的核心优势**：**不需要**将罚因子 $\sigma_k \to +\infty$ 就能保证算法的收敛性，从而避免了罚函数法的病态问题。

#### 定理2 & 定理3 (ALM的收敛性)
在温和的条件下（如Lagrange乘子序列有界、收敛点满足约束规范条件等），可以证明ALM产生的序列 $\{x^k\}$ 的聚点是原问题的KKT点。更进一步，如果罚因子 $\sigma_k$ 保持有界，算法通常表现出Q-线性收敛；如果 $\sigma_k \to \infty$，则能实现Q-超线性收敛。

### 针对一般约束问题的ALM
ALM可以推广到包含不等式约束 $g_j(x) \le 0$ 的问题。

1.  **引入松弛变量**：将不等式约束 $g_j(x) \le 0$ 转化为等式约束 $g_j(x) + s_j = 0$ 和非负约束 $s_j \ge 0$。
2.  **构造增广拉格朗日函数**：对新的等式约束构造增广拉格朗日函数，并将 $s_j \ge 0$ 用示性函数 $I_{\ge 0}(s)$ 加到目标函数中。
3.  **消去松弛变量**：对关于松弛变量 $s$ 的子问题进行显式求解，可以得到 $s$ 关于 $x$ 和乘子 $\mu$ 的表达式。
4.  **最终形式**：将 $s$ 的表达式代回，可以得到一个只关于 $x, \lambda, \mu$ 的等价的ALM迭代框架。其中不等式约束乘子的更新规则变为：
    $$
    \mu_j^{k+1} = \max\{ \mu_j^k + \sigma_k g_j(x^{k+1}), 0 \}
    $$

## 交替方向乘子法 (ADMM)
ADMM 是一种用于求解具有**可分离结构**的凸优化问题的强大算法。它结合了对偶分解和增广拉格朗日方法的优点。

### 问题背景
ADMM 主要用于解决如下形式的问题：
$$
\min_{x,y} f(x) + g(y) \quad \text{s.t.} \quad Ax + By = b
$$

**问题特点**：
*   **目标函数可分离**：目标函数是两个独立函数 $f(x)$ 和 $g(y)$ 的和。
*   **变量通过线性约束耦合**：变量 $x$ 和 $y$ 被一个线性的等式约束联系在一起。
*   $f, g$ 均是凸函数，但**不要求光滑**。

> 许多优化问题，包括复合型问题、约束问题等，都可以通过引入辅助变量转化为这种标准形式。例如，求解 $\min_x f(x) + g(x)$ 可以转化为 $\min_{x,y} f(x)+g(y)$ s.t. $x-y=0$。

### ADMM 算法
如果直接对上述问题应用ALM，需要在每一步联合最小化关于 $(x, y)$ 的增广拉格朗日函数，这可能很困难。ADMM 的核心思想是**交替地**、**逐个地**最小化这些变量。

**ADMM 迭代步骤**：
$$
\begin{align*}
x^{k+1} &:= \arg\min_x L_\sigma(x, y^k, \lambda^k) \\
y^{k+1} &:= \arg\min_y L_\sigma(x^{k+1}, y, \lambda^k) \\
\lambda^{k+1} &:= \lambda^k + \sigma(Ax^{k+1} + By^{k+1} - b)
\end{align*}
$$
其中 $L_\sigma(x, y, \lambda) = f(x) + g(y) + \lambda^\top(Ax+By-b) + \frac{\sigma}{2}\|Ax+By-b\|^2$ 是增广拉格朗日函数。

> **直观理解**：ADMM 将一个大的、联合的优化问题分解为两个小的、可以独立（交替地）求解的子问题。第一步固定 $y$ 和 $\lambda$ 来求解 $x$，第二步固定新的 $x$ 和旧的 $\lambda$ 来求解 $y$，最后再更新 $\lambda$。这种“逐个击破”的策略使得算法非常适合处理大规模和分布式问题。

#### 定理4 (ADMM的收敛性)
在相当宽松的条件下（$f, g$ 是闭凸函数，KKT解集非空），ADMM 算法可以保证收敛到原问题的一个KKT点。

### 案例：用ADMM求解LASSO问题

考虑 LASSO 问题：
$$
\min_x \frac{1}{2} \|Ax-b\|^2 + \lambda \|x\|_1
$$

**思路**：将它转化为ADMM的标准形式。
引入辅助变量 $y=x$，问题等价于：
$$
\min_{x,y} \underbrace{\frac{1}{2} \|Ax-b\|^2}_{f(x)} + \underbrace{\lambda \|y\|_1}_{g(y)} \quad \text{s.t.} \quad x - y = 0
$$
现在可以应用ADMM框架：
1.  **x-更新**：求解 $\arg\min_x \left( \frac{1}{2}\|Ax-b\|^2 + \frac{\sigma}{2}\|x-y^k+\lambda^k/\sigma\|^2 \right)$
    这是一个二次函数最小化问题（岭回归），有闭式解：
    $$
    x^{k+1} = (A^\top A + \sigma I)^{-1} (A^\top b + \sigma(y^k - \lambda^k/\sigma))
    $$
2.  **y-更新**：求解 $\arg\min_y \left( \lambda\|y\|_1 + \frac{\sigma}{2}\|x^{k+1}-y+\lambda^k/\sigma\|^2 \right)$
    这是一个 $L_1$ 范数的邻近算子问题，其解是著名的**软阈值算子 (soft-thresholding operator)**。
3.  **$\lambda$-更新**：
    $$
    \lambda^{k+1} = \lambda^k + \sigma(x^{k+1} - y^{k+1})
    $$

由于每个子问题都有高效的（甚至是闭式的）解，ADMM为求解LASSO问题提供了一个非常有效的算法。

## 分布式优化简介
ADMM在分布式计算环境中表现出色，尤其是在有中心协调节点的“星型”网络结构中。

### 问题背景
考虑如下分布式优化问题：
$$
\min_x \sum_{i=1}^N f_i(x) + g(x)
$$
*   $f_i(x)$ 是第 $i$ 个本地节点（或worker）上的损失函数，只依赖于本地数据。
*   $g(x)$ 是中心节点（或server）上的正则项。
*   所有节点共享同一个全局模型参数 $x$。

### 基于ADMM的分布式求解
**核心思想**：为每个本地节点引入一个“本地副本” $x_i$，并强制它们与中心节点的全局变量 $z$ 保持一致（**一致性约束**）。
$$
\min_{x_i, z} \sum_{i=1}^N f_i(x_i) + g(z) \quad \text{s.t.} \quad x_i - z = 0, \quad \forall i=1, \dots, N
$$
这是一个典型的ADMM可解结构。
**ADMM迭代过程**：
1.  **本地更新 (并行)**：每个本地节点 $i$ 并行地更新自己的 $x_i$：
    $$
    x_i^{k+1} := \arg\min_{x_i} \left( f_i(x_i) + \frac{\sigma}{2}\|x_i - z^k + \lambda_i^k/\sigma\|^2 \right)
    $$
    这个更新只依赖于本地数据 $f_i$ 和从中心节点广播下来的 $z^k, \lambda_i^k$。
2.  **中心更新**：本地节点将更新后的 $x_i^{k+1}$ 发送给中心节点。中心节点聚合信息并更新全局变量 $z$：
    $$
    z^{k+1} := \arg\min_z \left( g(z) + \sum_{i=1}^N \frac{\sigma}{2}\|x_i^{k+1} - z + \lambda_i^k/\sigma\|^2 \right)
    $$
3.  **乘子更新**：可以在本地或中心节点进行。

这种方式将复杂的全局优化问题分解为一系列可在本地并行处理的简单子问题和一个在中心的聚合步骤，极大地提高了计算效率，是联邦学习等现代分布式机器学习框架的核心算法思想之一。